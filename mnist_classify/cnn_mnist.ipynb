{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms                  # datasets用于加载数据集，transforms是各种变换\n",
    "from torch.utils.data import DataLoader, random_split         # DataLoader用于分批加载数据，random_split函数用于随机分割数据集\n",
    "import matplotlib.pyplot as plt     # 用于画图\n",
    "import torch.nn as nn               # 用于构建网络模型\n",
    "import torch.optim as optim         # 用于选择优化器\n",
    "import torch\n",
    "import numpy as np\n",
    "import os                           # 用于可视化错误样本\n",
    "from PIL import Image               # 用于可视化错误样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 选择是否使用GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要求：\n",
    "1. 选择使用gpu还是cpu\n",
    "2. 把三个地方的代码放在gpu上运算：模型、损失函数、数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")   \n",
    "print(\"当前使用的设备是：{}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据集处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 下载并加载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要求：\n",
    "\n",
    "知道使用datasets下载并加载常用公开数据集，以及加载自己的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载并加载数据集（官网上默认只有训练集和测试集，验证集需要自己划分）\n",
    "# 创建训练集对象\n",
    "train_dataset = datasets.MNIST( root=\"./\",                          # 数据集的存放地址\n",
    "                                download=True,                      # 如果本地没有，那么从官网下载数据集\n",
    "                                train=True,                         # 官网数据集分为6000张训练集，10000张测试集，这里加载训练集部分\n",
    "                                transform=transforms.ToTensor())    # 把从官网加载的数据转换成tensor的形式\n",
    "\n",
    "test_dataset = datasets.MNIST(  root=\"./\",                          # 数据集的存放地址\n",
    "                                download=True,                      # 如果本地没有，那么从官网下载数据集\n",
    "                                train=False,                        # 官网数据集分为6000张训练集，10000张测试集，这里加载测试集部分\n",
    "                                transform=transforms.ToTensor())    # 把从官网加载的数据转换成tensor的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 从训练集中分割出验证集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要求：\n",
    "1. 知道为什么要分割数据集， 训练集-测试集 的训练过程和 训练集-验证集-测试集 的区别\n",
    "2. 知道可以使用random_split和设置随机种子分割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从训练集中分割验证集\n",
    "\n",
    "# 定义训练集和验证集大小\n",
    "train_size = 55000  # 训练集大小\n",
    "val_size = 5000     # 验证集大小 (60000 - 55000 = 5000)\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed) # 设置PyTorch的随机种子，影响CPU上的随机数生成。\n",
    "    np.random.seed(seed)    # 设置NumPy的随机种子，因为random_split可能使用NumPy的随机数生成器。\n",
    "    if torch.cuda.is_available():   # 如果GPU可用，设置CUDA的随机种子。\n",
    "        torch.cuda.manual_seed(seed)    # 设置当前GPU的随机种子。\n",
    "        torch.cuda.manual_seed_all(seed)    # 如果使用多GPU，设置所有GPU的随机种子。\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 使用random_split划分训练集和验证集，random_split依赖上面随机种子的设置\n",
    "train_dataset, val_dataset = random_split(\n",
    "    train_dataset,\n",
    "    [train_size, val_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 可视化检验三个数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要求：\n",
    "1. print格式化输出数据\n",
    "2. 了解使用datasets加载的数据集的格式 \n",
    "3. 使用matplotlib.pyplot可视化图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检验三个数据集\n",
    "print(\"【查看数据集整体大小】\")\n",
    "print(\"训练集尺寸：{}\".format(len(train_dataset)))\n",
    "print(\"验证集尺寸：{}\".format(len(val_dataset)))\n",
    "print(\"测试集尺寸：{}\".format(len(test_dataset)))\n",
    "\n",
    "# 抽取一个样本查看详细内容\n",
    "print(\"【训练集查看一个样本点】\")\n",
    "img, label = train_dataset[0]\n",
    "print(\"训练集第一张图片的size:{}\".format(img.size()))\n",
    "plt.imshow(img.reshape(28,28), cmap=\"binary\")   # 显示图片，颜色映射为二值化图像\n",
    "plt.show()\n",
    "print(\"训练集第一张图片的label:{}\".format(label))\n",
    "\n",
    "print(\"【验证集查看一个样本点】\")\n",
    "img, label = val_dataset[0]\n",
    "print(\"验证集第一张图片的size:{}\".format(img.size()))\n",
    "plt.imshow(img.reshape(28,28), cmap=\"binary\")   # 显示图片，颜色映射为二值化图像\n",
    "plt.show()\n",
    "print(\"验证集第一张图片的label:{}\".format(label))\n",
    "\n",
    "print(\"【测试集查看一个样本点】\")\n",
    "img, label = test_dataset[0]\n",
    "print(\"测试集第一张图片的size:{}\".format(img.size()))\n",
    "plt.imshow(img.reshape(28,28), cmap=\"binary\")   # 显示图片，颜色映射为二值化图像\n",
    "plt.show()\n",
    "print(\"测试集第一张图片的label:{}\".format(label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 数据集分批"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要求：\n",
    "1. 使用DataLoader对数据分批\n",
    "2. 知道为什么训练集分批时要打乱顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批次大小\n",
    "batch_size = 64\n",
    "\n",
    "# 装载训练集\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)              # 训练时需要打乱顺序\n",
    "\n",
    "# 装载验证集\n",
    "val_loader = DataLoader(dataset=val_dataset,      \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)\n",
    "\n",
    "# 装载测试集\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 可视化检验每个dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要求：\n",
    "1. 了解dataloader对象的结构\n",
    "2. 能使用print检查dataloader内容\n",
    "3. 了解enumerate可以给可迭代对象添加索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检验每个dataloader的内容\n",
    "\n",
    "# train_loader某一个批次\n",
    "for i,data in enumerate(train_loader):  #遍历dataloader里面每一个批次，enumerate用于给一个可迭代对象添加索引\n",
    "    inputs,labels = data\n",
    "    if(i == 8): #这里改看第几个批次的数据\n",
    "        print(\"train_loader长度:{}个batch\".format(len(train_loader)))\n",
    "        print(\"train_loader第{}个批次:\".format(i))\n",
    "        print(\"图片size:{}\".format(inputs.shape))\n",
    "        print(\"标签size:{}\".format(labels.shape))\n",
    "        break  \n",
    "\n",
    "# val_dataset某一个批次\n",
    "for i,data in enumerate(val_loader):  #遍历dataloader里面每一个批次，enumerate用于给一个可迭代对象添加索引\n",
    "    inputs,labels = data\n",
    "    if(i == 8):\n",
    "        print(\"train_loader长度:{}个batch\".format(len(val_loader)))\n",
    "        print(\"val_dataset第{}个批次:\".format(i))\n",
    "        print(\"图片size:{}\".format(inputs.shape))\n",
    "        print(\"标签size:{}\".format(labels.shape))\n",
    "        break \n",
    "\n",
    "# test_dataset某一个批次\n",
    "for i,data in enumerate(test_loader):  #遍历dataloader里面每一个批次，enumerate用于给一个可迭代对象添加索引\n",
    "    inputs,labels = data\n",
    "    if(i == 8):\n",
    "        print(\"train_loader长度:{}个batch\".format(len(test_loader)))\n",
    "        print(\"test_dataset第{}个批次:\".format(i))\n",
    "        print(\"图片size:{}\".format(inputs.shape))\n",
    "        print(\"标签size:{}\".format(labels.shape))\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义模型结构、损失函数、优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 32, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2, 2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2, 2))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(64 * 7 * 7, 1000), nn.Dropout(p=0.4), nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(1000, 10), nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ([64, 1, 28, 28])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率（超参数）\n",
    "LR = 0.0003\n",
    "# 定义模型\n",
    "model = Net()\n",
    "model = model.to(device)    # 把模型放到gpu上\n",
    "# 定义代价函数\n",
    "entropy_loss = nn.CrossEntropyLoss()    # 内置了log_softmax方法，避免loss出现nan；内部使用独热编码计算等价形式，不用在外部显示把标签转换成独热编码\n",
    "entropy_loss = entropy_loss.to(device)  # 把损失函数放到gpu上\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义训练、验证、测试函数 和 错误样本可视化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 定义错误样本可视化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 错误样本可视化函数\n",
    "error_dir = \"./test_error\"  # 创建保存错误样本的文件夹\n",
    "os.makedirs(error_dir, exist_ok=True)\n",
    "\n",
    "# 保存错误分类的样本图像\n",
    "# 参数:\n",
    "# error_samples: 列表，每个元素是一个元组 (图像, 真实标签, 预测标签, 索引)\n",
    "# error_dir: 保存错误图像的目录路径\n",
    "def save_error_images(error_samples, error_dir):\n",
    "    print(f\"保存 {len(error_samples)} 个错误分类样本到 {error_dir}\")\n",
    "    \n",
    "    for img, true_label, pred_label, idx in error_samples:\n",
    "        # 创建图像\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        \n",
    "        # 显示图像\n",
    "        if len(img.shape) == 3 and img.shape[0] == 1:  # 如果是(1, 28, 28)的形状\n",
    "            img_display = img.squeeze(0)  # 去掉通道维度，变为(28, 28)\n",
    "        else:\n",
    "            img_display = img\n",
    "            \n",
    "        ax.imshow(img_display.cpu().numpy(), cmap='gray')\n",
    "        \n",
    "        # 设置标题\n",
    "        title = f\"True: {true_label}, Pred: {pred_label}\"\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        \n",
    "        # 不显示坐标轴\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # 保存图像\n",
    "        save_path = os.path.join(error_dir, f\"error_{idx:05d}_true{true_label}_pred{pred_label}.png\")\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=100)\n",
    "        plt.close(fig)  # 关闭图形以释放内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 定义训练、验证、测试函数\n",
    "\n",
    "要求：\n",
    "1. 了解训练时 前向传播--计算loss--反向传播--迭代参数 的训练流程\n",
    "2. 了解交叉熵损失函数的原理\n",
    "3. 验证时的流程： 前向传播--统计正确个数并计算准确率\n",
    "4. 使用张量的方法统计正确个数：直接比较两个张量--得到布尔张量--使用sum方法求和--使用item方法转换为整型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练-验证流程（训练阶段）：\n",
    "# 每一轮先在训练集上训练，训练完成后在验证集上验证\n",
    "# 输出在验证集上的accuracy，以及在训练集上的平均loss和accuracy\n",
    "\n",
    "# 训练函数\n",
    "def train():\n",
    "    model.train()       # 设置模型处于训练模式\n",
    "    total_loss = 0      # 统计训练总loss\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # （1）加载数据\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)    # 把训练数据放到gpu上\n",
    "        # （2）前向传播，获得模型预测结果，（64，10）\n",
    "        out = model(inputs)\n",
    "        # （3）计算loss\n",
    "        # 交叉熵代价函数out(batch,C),labels(batch)\n",
    "        loss = entropy_loss(out, labels)        # 这里不需要把标签转换成独热编码，CrossEntropyLoss会自动使用等价的方式计算loss\n",
    "        total_loss += loss\n",
    "        # （4）反向传播并迭代一次\n",
    "        # 梯度清0\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 修改权值\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_dataset)   # 返回平均loss（每张图片的loss）\n",
    "\n",
    "# 验证函数（在训练集和验证集上分别验证）\n",
    "def val():\n",
    "    model.eval()        # 设置模型处于验证模式\n",
    "    accuracy_val = 0    # 在验证集上的准确率\n",
    "    accuracy_train = 0  # 在训练集上的准确率\n",
    "    \n",
    "    # 在验证集上验证\n",
    "    correct_val = 0         # 正确的个数\n",
    "    for i, data in enumerate(val_loader):\n",
    "        # （1）加载数据\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)    # 把测试数据放到gpu上\n",
    "        # （2）前向传播，获得模型预测结果，（64，10）\n",
    "        out = model(inputs)\n",
    "        # 使用torch.max()把概率张量 转换成 所预测数字的张量predicted，尺寸是（64,1）\n",
    "        # torch.max() 第一个参数是输入张量；第二个参数表示：沿哪个维度求最大值（0表示按列，1表示按行）。\n",
    "        # torch.max() 返回一个元组，包含两个张量：第一个值：最大值本身;第二个值：最大值对应的索引位置\n",
    "        _, predicted = torch.max(out, 1)        \n",
    "        # （3）统计正确个数\n",
    "        # 统计预测正确的数量，两种方法实现\n",
    "        # 方法一：迭代比较\n",
    "        for i, predict in enumerate(predicted):\n",
    "            if(labels[i] == predict):\n",
    "                correct_val += 1\n",
    "        # 方法二：直接使用张量计算\n",
    "        # 一次性比较predicted和labels两个张量，得到一个布尔张量，.sum方法求和得到一个只有一个元素的张量。\n",
    "        # 严谨来说应该在后面继续使用.item()方法把张量类型转换成整型，但是这里自动转化了\n",
    "        # correct_val += (predicted == labels).sum().item()     \n",
    "    # 统计准确率\n",
    "    accuracy_val = correct_val / len(val_dataset)\n",
    "    \n",
    "   # 在训练集上验证\n",
    "    correct_train = 0         # 正确的个数\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # （1）加载数据\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)    # 把测试数据放到gpu上\n",
    "        # （2）前向传播，获得模型预测结果，（64，10）\n",
    "        out = model(inputs)\n",
    "        # 使用torch.max()把概率张量 转换成 所预测数字的张量predicted，尺寸是（64,1）\n",
    "        # torch.max() 第一个参数是输入张量；第二个参数表示：沿哪个维度求最大值（0表示按列，1表示按行）。\n",
    "        # torch.max() 返回一个元组，包含两个张量：第一个值：最大值本身;第二个值：最大值对应的索引位置\n",
    "        _, predicted = torch.max(out, 1)        \n",
    "        # （3）统计正确个数\n",
    "        # 统计预测正确的数量，方法二实现\n",
    "        # 方法二：直接使用张量计算\n",
    "        # 一次性比较predicted和labels两个张量，得到一个布尔张量，.sum方法求和得到一个只有一个元素的张量，.item()得到整型\n",
    "        correct_train += (predicted == labels).sum().item()     \n",
    "    # 统计准确率\n",
    "    accuracy_train = correct_train / len(train_dataset)\n",
    "\n",
    "    # 返回\n",
    "    return accuracy_val, accuracy_train\n",
    "\n",
    "# 测试函数（在测试集上验证,包含保存错误样本）\n",
    "def test():\n",
    "    model.eval()        # 设置模型处于验证模式\n",
    "\n",
    "    # 在验证集上验证\n",
    "    correct_test = 0         # 正确的个数\n",
    "    error_samples = []       # 保存错误样本\n",
    "    for i, data in enumerate(test_loader):\n",
    "        # （1）加载数据\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)    # 把测试数据放到gpu上\n",
    "        # （2）前向传播，获得模型预测结果，（64，10）\n",
    "        out = model(inputs)\n",
    "        # 使用torch.max()把概率张量 转换成 所预测数字的张量predicted，尺寸是（64,1）\n",
    "        # torch.max() 第一个参数是输入张量；第二个参数表示：沿哪个维度求最大值（0表示按列，1表示按行）。\n",
    "        # torch.max() 返回一个元组，包含两个张量：第一个值：最大值本身;第二个值：最大值对应的索引位置\n",
    "        _, predicted = torch.max(out, 1)        \n",
    "        # （3）统计正确个数，并记录错误样本\n",
    "        for j in range(len(labels)):    #遍历这个批次的每一个样本\n",
    "            if labels[j] == predicted[j]:\n",
    "                correct_test += 1\n",
    "            else:\n",
    "                # 收集错误样本：图像、真实标签、预测标签、全局索引\n",
    "                img_idx = i * batch_size + j    # 图像的绝对标签，整个数据集中的第几张\n",
    "                error_samples.append((inputs[j], labels[j].item(), predicted[j].item(), img_idx))\n",
    "    \n",
    "    # 统计准确率\n",
    "    print(f\"在测试集上的精度是:{correct_test / len(test_dataset)}\")\n",
    "\n",
    "    # 保存错误样本\n",
    "    save_error_images(error_samples, error_dir)\n",
    "    \n",
    "    # 返回准确率\n",
    "    return correct_test / len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 主函数：训练-验证-测试全流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_path = \"./models/best.pth\"        # 定义模型保存路径\n",
    "best_val_acc = 0                    # 验证集上的最高精度\n",
    "\n",
    "# 训练-验证流程\n",
    "for epoch in range(0, 20):   # 此处调整训练轮数\n",
    "    print('epoch:',epoch)\n",
    "    avg_loss = train()\n",
    "    accuracy_val, accuracy_train = val()\n",
    "    print(f\"训练集上的loss是:{avg_loss}, accuracy是:{accuracy_train}\")\n",
    "    print(f\"验证集上的accuracy是:{accuracy_val}\")\n",
    "\n",
    "    # 保存验证集上精度最高的模型\n",
    "    if accuracy_val > best_val_acc:\n",
    "        best_val_acc = accuracy_val\n",
    "        # 保存模型参数\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': accuracy_val,\n",
    "            'train_acc': accuracy_train\n",
    "        }, best_model_path)\n",
    "        print(f'  ✓ 保存最佳模型，验证集准确率: {accuracy_val:.2f}%')\n",
    "\n",
    "# 测试流程\n",
    "# 加载最佳模型，在测试集上测试精度\n",
    "print('\\n加载最佳模型并在测试集上测试...')\n",
    "checkpoint = torch.load(best_model_path)                 # checkpoint包含很多参数\n",
    "model.load_state_dict(checkpoint['model_state_dict'])    # 取其中的模型参数加载\n",
    "# 开始测试\n",
    "test_acc = test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
